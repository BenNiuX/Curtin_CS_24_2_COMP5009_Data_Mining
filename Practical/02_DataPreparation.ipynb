{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-DaMSaMbskG"
      },
      "source": [
        "# Practical 02 - Data Preparation\n",
        "In this practical we'll be using Python/[Pandas](https://pandas.pydata.org) to explore and visualise some example data using [Jupyter Notebooks](https://jupyter.org) in [Google Colaboratory](https://colab.research.google.com).\n",
        "\n",
        "At the start of each practical you should make a copy of the notebook in your own google drive:\n",
        "- File -> Save as copy in Drive\n",
        "\n",
        "If you would prefer to work with a Jupyter Lab session ony your own machine you can download the notebook directly via:\n",
        "- File -> Downaload -> Download .ipynb\n",
        "\n",
        "In this practial we will be preparing data for use with some data mining applications.\n",
        "\n",
        "The data for this week can be found on [GitHub](https://github.com/PaulHancock/COMP5009_pracs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titanic revisited\n",
        "\n",
        "Last week we explored the titanic manifest dataset using SQL.\n",
        "This week we'll start by re-doing much of this exploration as we learn to use python, pandas, and jupyter notebooks.\n",
        "\n",
        "Complete the following tasks:\n",
        "1. Load the data file [titanic.csv](https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/titanic.csv) from GitHub.\n",
        "2. Display the first 10 rows of the dataset.\n",
        "3. Examine the column names and content and determine an appropriate data type for each.\n",
        "  - Look at the data types used by pandas and comment on any differences\n",
        "4. Is the data multidimensional?  \n",
        "  - Find the number of attributes and the number of instances.\n",
        "5. For each attribute:\n",
        "  - check if there are any missing entries\n",
        "  - Find the min, max, avg, sum values if relevant\n",
        "  - Plot a histogram of the data\n",
        "6. Select all the passengers with \"Dr.\" in their name.\n",
        "  - Group these data by sex\n",
        "7. Compute the average fare grouped by class and embarkation port.\n",
        "  "
      ],
      "metadata": {
        "id": "0gSLFhVfswBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the modules that we'll be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "# np, pd, and plt are common shorthands for these often used modules"
      ],
      "metadata": {
        "id": "d_CH1U5wuTSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the data\n",
        "Load the data into a pandas data frame"
      ],
      "metadata": {
        "id": "UGXB4LeyuhsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The data are hosted in github and we can load it directly from the URL\n",
        "data_url = 'https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/titanic.csv'\n",
        "tt_df = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "UnLqpWJAup6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view the loaded data to make sure that it makes sense\n",
        "tt_df"
      ],
      "metadata": {
        "id": "0tD4NxlKuzS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Display the first 10 rows\n",
        "We can make use of the `.head()` method provided by the data frame.\n",
        "Try also:\n",
        "- df.head(5)\n",
        "- df.tail()"
      ],
      "metadata": {
        "id": "dhvuU7A7vJGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tt_df.head()"
      ],
      "metadata": {
        "id": "ZyuZh7F7vSE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Examine the data\n",
        "Examine the column names and content and determine an appropriate data type for each.\n",
        "  - Look at the data types used by pandas and comment on any differences"
      ],
      "metadata": {
        "id": "DAD0VcVtvbok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Column Names\")\n",
        "print(tt_df.columns)\n",
        "print()\n",
        "print(\"Data types\")\n",
        "print(tt_df.dtypes)"
      ],
      "metadata": {
        "id": "ru9bGw6-voCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Is the data multidimensional?  \n",
        "Find the number of attributes and the number of instances."
      ],
      "metadata": {
        "id": "95gRWw6Nv274"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tt_df.shape"
      ],
      "metadata": {
        "id": "FyekfED4v5lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Summarise the data\n",
        "For each attribute:\n",
        "  - check if there are any missing entries\n",
        "  - Find the min, max, avg, sum values if relevant\n",
        "  - Plot a histogram of the data"
      ],
      "metadata": {
        "id": "JZw8ynBjwHqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tt_df.describe()"
      ],
      "metadata": {
        "id": "mZLvw3uSv8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if we run a funciton on the data frame it runs on ech column.\n",
        "# `numeric_only=True` means that we ignore the text columns\n",
        "tt_df.sum(numeric_only=True)"
      ],
      "metadata": {
        "id": "rXEM4HsjwO9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can plot a histogram of all the data together\n",
        "tt_df.hist(figsize=(12,12))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRzcxtUGwhl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or one by one\n",
        "tt_df['Age'].hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TV-TcUw0xRnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Find the doctors\n",
        "Select all the passengers with \"Dr.\" in their name.\n",
        "- Group these data by sex"
      ],
      "metadata": {
        "id": "zvl0bvJsxmOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in sql we have a filter using the WHERE clause to select our data\n",
        "filter = tt_df['Name'].str.contains('Dr\\.')\n",
        "doctors = tt_df[filter]\n",
        "# then we GROUP BY, and then we run the count() command\n",
        "# there is no pandas equivalent of count(*) so we have to run count on a column with no NULLs\n",
        "doctors.groupby('Sex')['Name'].count()"
      ],
      "metadata": {
        "id": "Q9QtNhTLxp70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Average fare\n",
        "Compute the average fare grouped by class and embarkation port."
      ],
      "metadata": {
        "id": "19-b6IFUzMVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remeber the ordering:\n",
        "# group, select columns, compute\n",
        "tt_df.groupby(['Pclass','Embarked'])['Fare'].mean()"
      ],
      "metadata": {
        "id": "4Ljc8p2azQdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqWJd87HcgLF"
      },
      "source": [
        "## Q3 from Chapter 2 of [Aggarwal](https://www.springer.com/gp/book/9783319141411)\n",
        "\n",
        "We will be working with the Arrythmia data set from the UCI Machine Learning Repository at http://archive.ics.uci.edu/ml/datasets/arrhythmia. The data are available via github as [arrhythmia.data.with.header.csv](https://github.com/PaulHancock/COMP5009_pracs/blob/main/data/arrhythmia.data.with.header.csv)\n",
        "\n",
        "Use Pandas to load the data and complete the following tasks:\n",
        "1. Load the data into a pandas data frame, converting '?' into `NaN`\n",
        "\n",
        "1. Find and remove all attributes with more than 80% missing values.\n",
        "\n",
        "1. Detect all duplicate rows and remove them if found.\n",
        "\n",
        "1. Find all attributes with less than 5% missing values and replace these missing values with either the mean or the mode of the attribute.  \n",
        "\n",
        "1. Discretize attributes att3 and att4 into 10 equi-width ranges and 10 equi-depth ranges respectively.\n",
        "\n",
        "  a. Examine and comment on the intervals in each case.\n",
        "\n",
        "1. Standardize all numeric attributes to a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "1. Create a database `arrythmia.db`.\n",
        "  \n",
        "  a. Randomly sample 100 instances and save them to a table called `test`.\n",
        "\n",
        "  b. Save the remaining instances to a table called `train`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl36oq-AesTg"
      },
      "source": [
        "### Load the data\n",
        "Load the data into a pandas data frame, converting '?' into `NaN`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWmUd1v4Klom"
      },
      "source": [
        "Look at last week's prac and load the data into our data frame.\n",
        "\n",
        "The link to the data file should be: https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/arrhythmia.data.with.header.csv\n",
        "\n",
        "The difference here is that the csv file uses '?' to indicate missing values. This is annoying but pandas has a workaround.\n",
        "Look at the help for the `pd.read_csv` to see what parameter you might use to indicate that '?' should be interpreted as `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFC2rsTqyN_P"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m43xXuZe366"
      },
      "source": [
        "# load the data into our data frame\n",
        "data_url = 'https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/arrhythmia.data.with.header.csv'\n",
        "ar_df = pd.read_csv(data_url, na_values='?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja8VH3vNyYEV"
      },
      "source": [
        "ar_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFep6rzh122n"
      },
      "source": [
        "### Remove attributes\n",
        "Find and remove all attributes with more than 80% missing values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-xxQSfU1-AN"
      },
      "source": [
        "We could use `df.describe()` however there are so many columns that we need an automated approach. Thus we will write a function to compute the fraction of missing rows, and print the name and fraction for each columnn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCzU_hqygp1"
      },
      "source": [
        "# find which columns have missing data\n",
        "def missing(df):\n",
        "  \"\"\"\n",
        "  For each attribute/column in the dataframe `df`, count the number of missing entries.\n",
        "  Return a list of all the coulmns with more than 80% missing entries.\n",
        "  \"\"\"\n",
        "  missing_dict = dict()\n",
        "  total = df.shape[0] # shape[0] is the number of rows\n",
        "  for attribute in df.columns:\n",
        "    missing = df[attribute].isna().sum() # count the number of Null/nan/na values\n",
        "    frac = missing/total * 100 # as a percentage\n",
        "    missing_dict[attribute] = frac\n",
        "  return missing_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUnjdYQc40l7"
      },
      "source": [
        "m_dict = ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at each attribute/frac pair in the dict and choose those with a frac that is >80\n",
        "cols_to_drop = [ att for att,frac in m_dict.items() if ?]\n",
        "cols_to_drop"
      ],
      "metadata": {
        "id": "9LjdTKW9QwHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO-If4uI2P94"
      },
      "source": [
        "Deleting the columns can be done with the `.drop()` function, we just need a list of columns to remove."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oD_4w27v2XZO"
      },
      "source": [
        "# figure out which columns you want to drop from above, and put their names in the list below\n",
        "ar_df.drop(columns=cols_to_drop,\n",
        "           inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2vr5IkD3tbt"
      },
      "source": [
        "# confirm that our data frame now has fewer columns (was 280)\n",
        "ar_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMXKr5aPC8oa"
      },
      "source": [
        "### Detect and remove duplicates\n",
        "Detect all duplicate rows and remove them if found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nW1IV6oDv8V"
      },
      "source": [
        "Look at `duplicated` as a starting point. Try to show all duplicates (the ones that we will remove)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlxj3MlCr5J"
      },
      "source": [
        "dups = ar_df.duplicated()\n",
        "dups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dups variable is a series with too many rows to display so instead we can count the number of True values by using `sum()`"
      ],
      "metadata": {
        "id": "ix2EZWAknAIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recall that true = 1, false = 0\n",
        "dups.sum()"
      ],
      "metadata": {
        "id": "UpzfXa6anHUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If there are no nuplicates then skip to the next problem."
      ],
      "metadata": {
        "id": "UymRZKjIKOUg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lUQzkhkEDwl"
      },
      "source": [
        "Removing duplicate rows can be done either by noting the index of the duplicates above and feeding that into the `drop()` function, or by using the `drop_duplicates()` function which combines the finding and dropping together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ugkk5-NDEuH"
      },
      "source": [
        "# using ~dups we are indexing on all rows of dups that are false (ie not duplicates)\n",
        "ar_df[~ dups]\n",
        "\n",
        "# lets use drop_duplicates() to remove the duplicate rows\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6tWiY1m4qEe"
      },
      "source": [
        "### Replace missing values\n",
        "Find all attributes with less than 5% missing values and replace these missing values with either the mean or the mode of the attribute."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at each attribute/frac pair in the dict and choose those with a frac that is <5\n",
        "cols_to_impute = [ att for att,frac in m_dict.items() if ? ]\n",
        "cols_to_impute"
      ],
      "metadata": {
        "id": "T6jbNsTG3mh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC1Bwv-444Ya"
      },
      "source": [
        "Look at the `fillna()` function for ways to replace the `NaN` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_gRPSCs5D7k"
      },
      "source": [
        "for col in ?:\n",
        "  # compute the mean\n",
        "  mean = ar_df[col].mean()\n",
        "  # now use the fillna function to replace the NaN avalues with the mean value\n",
        "  ar_df[col].fillna(mean, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the replacement has worked, by re making our m_dict\n",
        "m_dict = ?\n",
        "for col in cols_to_impute:\n",
        "  print(col, \"missing data\", ?)"
      ],
      "metadata": {
        "id": "9MCBC84zLIOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZ7iOHp7cWb"
      },
      "source": [
        "### Discritize attributes\n",
        "Discretize attribute att4 into 10 equi-width ranges and 10 equi-depth ranges respectively.\n",
        "\n",
        "a. Examine and comment on the intervals in each case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s0YiUlABy33"
      },
      "source": [
        "First lets get a summary of the attribute so that we can see what we are dealing with here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n1G4tXPBt7v"
      },
      "source": [
        "ar_df['att4'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WHJLyaJ7mYR"
      },
      "source": [
        "Look at the documentation for `qcut` and `cut` for help with this one."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will display help for the pd.cut function in the browser (this is Jupyter magic, not part of normal python)\n",
        "?pd.cut"
      ],
      "metadata": {
        "id": "rQIyaQyWnXQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMfDr4755FhV"
      },
      "source": [
        "# now use qcut to create equal depth bins (or at least try)\n",
        "# use 10 quantiles\n",
        "# use integers for bin labels\n",
        "# drop the duplicate bin edges\n",
        "# return the bin edges\n",
        "# look at the help for qcut to figure out what parameters to pass\n",
        "results, bins = pd.cut ?\n",
        "\n",
        "print(\"bin name | number of entries\")\n",
        "print(results.value_counts(sort=False))\n",
        "\n",
        "print(\"The bin edges\")\n",
        "print(bins)\n",
        "\n",
        "att3_10_equi_width = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYvxhcfZ7yaM"
      },
      "source": [
        "# plot a histogram of the equiwidth data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YmFp76H-BDJ"
      },
      "source": [
        "# now use qcut to create equal depth bins (or at least try)\n",
        "# look at the help for qcut to figure out what parameters to pass\n",
        "results, bins = pd.qcut ?\n",
        "\n",
        "print(\"bin name | number of entries\")\n",
        "print(results.value_counts(sort=False))\n",
        "\n",
        "print(\"The bin edges\")\n",
        "print(bins)\n",
        "att3_10_equi_depth = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x_tIw4K8uX7"
      },
      "source": [
        "# plot a histogram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrHTWTDaCH4K"
      },
      "source": [
        "### Standardise numeric attributes\n",
        "Standardize all numeric attributes to a mean of 0 and a standard deviation of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL8hdQgHDVXG"
      },
      "source": [
        "So for each atttribute compute the mean ($\\mu$) and the standard deviation ($\\sigma$) and then replace the values with $z= \\frac{x-\\mu}{\\sigma}$\n",
        "\n",
        "`sklearn` has a `preprocessing` package that has a `StandardScaler` that we can use for this.\n",
        "\n",
        "Note: There is also a `MinMaxScaler` that works in the same way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "Reso5ito_4J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose all the numeric type attributes\n",
        "numeric_attributes = ar_df.select_dtypes(include=?).columns\n",
        "numeric_attributes"
      ],
      "metadata": {
        "id": "ztJwFmM5SgaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a standard scaler and fit it to our data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(?)\n",
        "\n",
        "# Now transform our data using this scaler, replacing the original data\n",
        "ar_df[numeric_attributes] =  scaler.transform(ar_df[numeric_attributes])\n",
        "# if you have other data frames that you want to scale with the same transform you can do it as\n",
        "# other_df[numeric_attributes] = scaler.transform(other_df[numeric_attributes])"
      ],
      "metadata": {
        "id": "sG0tMflR_wNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify that this worked\n",
        "# the mean should be close to zero, and the std should be close to 1.\n",
        "ar_df.describe()"
      ],
      "metadata": {
        "id": "1BVFTWanSc1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmRVqFBhEfFK"
      },
      "source": [
        "### Random sampling\n",
        "Create a database arrythmia.db.\n",
        "\n",
        "a. Randomly sample 100 instances and save them to a table called test.\n",
        "\n",
        "b. Save the remaining instances to a table called train."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3 as sql"
      ],
      "metadata": {
        "id": "hXFZwZlu5mRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to a database on disk with a given name\n",
        "# check the file browser on the left to see that the file has been created.\n",
        "con = sql.connect('arrythmia.db')"
      ],
      "metadata": {
        "id": "ptjt9xWj5pMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmpegtzGkfJ"
      },
      "source": [
        "Look at `sample` to generate a random selection of rows/instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y22r0aJaEljd"
      },
      "source": [
        "test = ar_df.sample(n=?)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJKcY5UrGqru"
      },
      "source": [
        "Now the inverse problem. Not so easy as there is no pandas function that will directly give us the inverse of the above.\n",
        "\n",
        "Instead we can remove the test data from the full data set to give us the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DYnYCUnGQrt"
      },
      "source": [
        "train = ar_df.drop(test.index)\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZHKwx8Hf-6"
      },
      "source": [
        "we save our tables to our database.\n",
        "\n",
        "We *don't* want the pandas index as it's not a useful value to save.\n",
        "\n",
        "We want to save each data frame to a different table in the databse so we must provide a name.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-K-R6CxHfkT"
      },
      "source": [
        "# use test/train as the table names\n",
        "# don't include the pandas index in the database\n",
        "test.to_sql( ? )\n",
        "train.to_sql( ? )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYBEcOccHyeX"
      },
      "source": [
        "Look in the current directory to see these files and download them. On the left panel click the file explorer to see the files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIH1yANcIdr-"
      },
      "source": [
        "## Q13 from Chapter 3 of [Aggarwal](https://www.springer.com/gp/book/9783319141411)\n",
        "\n",
        "Use the modified KDD Cup 1999 data set provided as `kddcup.csv` and specifically examine attribute `count`.\n",
        "\n",
        "1. The data are available via github as [kddcup.arff](https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/kddcup99.arff), load them as a pandas data frame\n",
        "\n",
        "1. Compute the average µ and standard deviation σ of the `count` attribute over 10,000 samples.\n",
        "\n",
        "1. Randomly select a subset of n samples from this data set with\n",
        "  n = 10; 20; 50; 100; 200; 500; 1000; 2000; 5000; 10000:\n",
        "\n",
        "  For each value of n compute the average $e_n$ of the attribute over the subset and then derive the following quantity:\n",
        "\n",
        "  $z_n=\\frac{|e_n-\\mu|}{\\sigma}$\n",
        "\n",
        "  You should repeat this at least 10 times and obtain the average of zn.\n",
        "\n",
        "1. Plot $z_n$ versus n and make a comment on the graph you have plotted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPg_AA3-xrYE"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "The data are available via github as [kddcup.csv](https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/kddcup99.arff), load them as a data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UERaE2hQpy6Q"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# This time the file is in arff format so we need a library that can read it\n",
        "from scipy.io import arff\n",
        "import urllib\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w_ii-GAsY5N"
      },
      "source": [
        "The file that we are working with is in `arff` format, which can't be read directly by pandas. Instead we use an arff loader from `scipy.io`, but we must first download the file before we can open it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgxH9WQqspht"
      },
      "source": [
        "data_url = 'https://raw.githubusercontent.com/PaulHancock/COMP5009_pracs/main/data/kddcup99.arff'\n",
        "file_name = 'kddcup99.arff'\n",
        "# this will download the file, look in your explorer to confirm\n",
        "urllib.request.urlretrieve(data_url, file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgh_Jwx2p9LG"
      },
      "source": [
        "# load the data from arff format\n",
        "data = arff.loadarff(file_name)\n",
        "kd_df = pd.DataFrame(data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6X0Faqos8kk"
      },
      "source": [
        "### Compute $\\mu$ and $\\sigma$\n",
        "Compute the average µ and standard deviation σ of the `count` attribute over 10,000 samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il_DF_NDqE8G"
      },
      "source": [
        "mu = ?\n",
        "sigma = ?\n",
        "print(f\"For attribute `count`: μ={mu:.2f}, σ={sigma:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEIuyqlthfI"
      },
      "source": [
        "### Random subsets\n",
        "Randomly select a subset of n samples from this data set with\n",
        "  n = 10; 20; 50; 100; 200; 500; 1000; 2000; 5000; 10000:\n",
        "\n",
        "  For each value of n compute the average $e_n$ of the attribute over the subset and then derive the following quantity:\n",
        "\n",
        "  $z_n=\\frac{|e_n-\\mu|}{\\sigma}$\n",
        "\n",
        "  You should repeat this at least 10 times and obtain the average of zn.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3idqm5HtyiZ"
      },
      "source": [
        "x = [10,20,50,100,200,1_000,2_000,5_000,10_000]\n",
        "y = []\n",
        "for n in x:\n",
        "  # append to z for 10 iterations\n",
        "  z = []\n",
        "  for _ in range(10): # repeat 10 times to get the average of zn\n",
        "    # compute e_n, and then z onsubsets of size n\n",
        "    subset =\n",
        "    en = subset.mean()\n",
        "    z.append(abs(en-mu)/sigma)\n",
        "  # compute the average of zn and append to y\n",
        "  zn = np.mean(z)\n",
        "  y.append(zn)\n",
        "  print(f'n={n}, zn={zn}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAxORIq0xCrW"
      },
      "source": [
        "### Plot $z_n$\n",
        "Plot $z_n$ versus n and make a comment on the graph you have plotted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2jWnCg_xOEW"
      },
      "source": [
        "Let's use matplotlib directly for our plotting this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htswyvPmuK64"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1bRFvWfvE6Z"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.plot(x,y)\n",
        "#ax.set_yscale(?)\n",
        "ax.set_xlabel('Number of samples')\n",
        "ax.set_ylabel('Mean of samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4RbOqgVxSgu"
      },
      "source": [
        "What do we observe in the above?\n",
        "\n",
        "Hint: try a log scale for the axes."
      ]
    }
  ]
}