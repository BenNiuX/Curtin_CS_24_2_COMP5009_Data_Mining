{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical 06 - Regression\n",
        "\n",
        "\n",
        "As usual, you should save a copy of this notebook in Google drive (or on your own system if not using Colaboratory)\n",
        "\n",
        "In this session we will:\n",
        "- load and explore a data set from astronomy\n",
        "- Perform feature selection, feature engineering, and data scaling\n",
        "- Train three models on our data:\n",
        "  - A KNN regressor\n",
        "  - A linear regression model\n",
        "  - A descision tree regressor\n",
        "- Compare and contrast the results from our models."
      ],
      "metadata": {
        "id": "TzxWs-azH3xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data reading and manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# plotting libraries\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Machine learning tools\n",
        "import sklearn\n",
        "from sklearn import (metrics, preprocessing, linear_model, model_selection)\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "print(f\"Numpy:        {np.__version__}\")\n",
        "print(f\"Pandas:       {pd.__version__}\")\n",
        "print(f\"Matplotlib:   {matplotlib.__version__}\")\n",
        "print(f\"Seaborn:      {sns.__version__}\")\n",
        "print(f\"Scikit-learn: {sklearn.__version__}\")"
      ],
      "metadata": {
        "id": "EmCfkFGDGclb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "MlIPgj7JQs5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be working with a data set that comes from astronomy.\n",
        "The data have been downloaded from an archive filtered to be relevant for this practical.\n",
        "We can pull the data directly from my GitHub."
      ],
      "metadata": {
        "id": "SSoAOqFzKHsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "galaxy_df = pd.read_csv(\"https://raw.githubusercontent.com/PaulHancock/2023-ASA-ML-DeepDive/main/SDSS_10k_Galaxy.csv\")"
      ],
      "metadata": {
        "id": "SaK5AWp-H9k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "galaxy_df.columns"
      ],
      "metadata": {
        "id": "7mARx5IyKaXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we can see the following columns:\n",
        "- objid: the object identifier, an integer\n",
        "- ra, dec: the location of the object on the sky (ra/dec = lat/long for the sky)\n",
        "- u,g,r,i,z: photometric observations. How bright an object is in each part of the spectrum, measured in magnitudes, which is a logarithmic scale.\n",
        "- run, rerun, camcol, field, specobjid, plate, fiberid: meta-data associated with the observation. Not specific to the object but descirbes the setup of the instrument during the observaiton.\n",
        "- redshift: the redshift of the object being observed. This is a measure of distance for astronomers.\n",
        "- mjd: the date of the observaiton\n",
        "-SpType, BV, TEff, FeH: properties of the object which are all null because these are only valid for stars and our data has been filtered to include only galaxies."
      ],
      "metadata": {
        "id": "TSLdPYu1KYKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "galaxy_df.describe()"
      ],
      "metadata": {
        "id": "medGnys-Giuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature selection"
      ],
      "metadata": {
        "id": "aWFPvRa2QwJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will remove all the \"non-physical\" properties from our dataset, and our goal is to use the (u,g,r,i,z) photometric magnitudes to predict the redshift of the galaxies.\n",
        "\n",
        "In astronomy, the redshift is measured by taking a spectrum of an object, and this typically takes a long time to measure using large and very expensive telescopes. The photometric magnitude of an object can be measured relatively quickly, so being able to predict redshifts from photometry means that we could use a lot less time, or by using smaller and less expensive instruments."
      ],
      "metadata": {
        "id": "DL0sT49nLdbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "galaxy_df = galaxy_df[ ? ]"
      ],
      "metadata": {
        "id": "wSkpz9VtGlFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data scaling"
      ],
      "metadata": {
        "id": "b_cHP6sbQ0uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data are calibrated to a physically meaningful scale, however the different magnitudes don't share the same distributions."
      ],
      "metadata": {
        "id": "ogwAblu-MfWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,3, figsize=(12, 8))\n",
        "galaxy_df.hist(grid=True, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NlT9SmPKGu0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to eliminate issues with scaling we will transform all our features using a min/max scaler.\n",
        "Some of the features look like they may have a normal or power-law distribution, so min/max might not be the best solution, but this is what we'll go with for now.\n",
        "You can come back and try different scalers at a later time."
      ],
      "metadata": {
        "id": "-UUPYYB6UPIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocessing classes have a similar interface to the ML models:\n",
        "- `.fit` to fit the transformer to your data\n",
        "- `.transform` to return the transformed data\n",
        "- `.fit_transform` to do both at once"
      ],
      "metadata": {
        "id": "ZJtuutxtoCcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a minmax scaler\n",
        "minmax = preprocessing.MinMaxScaler()\n",
        "# \"fit\" the scaler -> determine the min/max for each column\n",
        "minmax.fit(galaxy_df)\n",
        "\n",
        "#make a copy of the data frame we are about to scale\n",
        "scaled_df = galaxy_df.copy()\n",
        "# Transform the data and save it back to our dataframe\n",
        "scaled_df[:] = minmax.transform(scaled_df)\n",
        "#        ^ this slicing hack causes the values of the pandas dataframe to\n",
        "# be overwritten with the values from a numpy array (from the tranform)"
      ],
      "metadata": {
        "id": "pYZHJIP-G0vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Look at our scaled data to check the effects of our code.\n",
        "\n",
        "You should see all columns have a $min = 0$ and $max = 1.0$"
      ],
      "metadata": {
        "id": "8HD2oovDwx8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df.describe()"
      ],
      "metadata": {
        "id": "IqUTYjtzHctl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more compact way to visualise the spread of data within our range $[0,1]$ is to use a box and whisker plot.\n",
        "\n",
        "Seaborn gives us a fairly nice way to do this, we just have to make sure that we fiddle the axis labels so they don't overlap."
      ],
      "metadata": {
        "id": "Dg6ENTP4XbDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.boxplot(scaled_df)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hxLBX0i_HhOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a full view of the correlation of all parameters we can use the pandas `.corr()` method to compute the correlation and then the seaborn `.heatmap()` method to display it."
      ],
      "metadata": {
        "id": "nOj3VTh1Xom8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation between each of the numeric parameters as well as the\n",
        "# target attribute\n",
        "cor = scaled_df.corr()\n",
        "\n",
        "# use seaborn to do the plot\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.rainbow, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1zxa6Px_H3TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Is correlation the best way to judge the utility of a feature?\n",
        "\n",
        "What assumptions are we making here?\n",
        "\n",
        "How could we make different assumptions?"
      ],
      "metadata": {
        "id": "fdUoBAc6p-xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "oG4F0MIxQosA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A commonly used feature that astronomers use to classify galaxies into different types are the spectral shapes (or colours) of the galaxy.\n",
        "These colours are ratios of brightness levels, which correspond to linear combinations of magnitudes.\n",
        "\n",
        "Let us add some additional features which are the pariwise differences between neighbouring bands.\n",
        "\n",
        "Note that we need to compute the difference in the raw data first so that we are working with physically meaningful features. We then scale this with our minmax scaler."
      ],
      "metadata": {
        "id": "olqDTa5DNNRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = scaled_df.drop(columns='redshift')\n",
        "# Create colours as our new features\n",
        "X['u-g'] = galaxy_df['u'] - galaxy_df['g']\n",
        "X['g-r'] = ?\n",
        "X['r-i'] = ?\n",
        "X['i-z'] = ?\n",
        "# Once computed we can scale the new features so that they also have a range of [0,1]\n",
        "X[ ['u-g','g-r','r-i','i-z'] ] = minmax.fit_transform( ? )\n",
        "\n",
        "y = galaxy_df['redshift'] # we want unscaled reshifts to be predicted"
      ],
      "metadata": {
        "id": "tJhuWWmRIHi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation between each of the numeric parameters as well as the\n",
        "# target attribute\n",
        "cor = X.?\n",
        "\n",
        "# use seaborn to do the plot\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.rainbow, ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2YuanjX8YlGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the correlation map above we can see examples of:\n",
        "- attribtes which have a correlation of ~1,\n",
        "- attributes which are highly correlated with the redshift,\n",
        "- attributes which have very low correlation with the redshift.\n",
        "\n",
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "In each of the above cases, how does this inform our feature selection?"
      ],
      "metadata": {
        "id": "itWeoyv4Yiho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Create a test/train split from our `X` data.\n"
      ],
      "metadata": {
        "id": "v_IGSzQjPcDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our data into a test/train set\n",
        "X_train, X_test, y_train, y_test = ?"
      ],
      "metadata": {
        "id": "x5md3mw6IYMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN regressor"
      ],
      "metadata": {
        "id": "P7c-yHY1Ql2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do our gridsearch as before however we can no longer use 'accuracy' as a metrics, since we don't have categorical data.\n",
        "\n",
        "Instead we need to use a different scoring metric.\n",
        "\n",
        "For now let's use the $R^2$ metric since it's easy do understand.\n",
        "\n",
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Perform a gridsearch with cross validation using the `r2` metric for scoring.\n",
        "\n"
      ],
      "metadata": {
        "id": "35tOOA8uN_mW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all the parameters we'll be iterating over\n",
        "parameters = {'weights': (?), # this should be the different weighting schemes\n",
        "              'n_neighbors':[?]} # this should be a list of the nearest neigbhours\n",
        "# make a classifier object\n",
        "estimator = ?\n",
        "# create a GridSearchCV object to do the training with cross validation\n",
        "gscv = model_selection.GridSearchCV(estimator=?,\n",
        "                                    param_grid=parameters,\n",
        "                                    scoring=?)\n",
        "# now train our model\n",
        "knn = ?\n",
        "knn_y_pred = ?\n",
        "print(knn.best_params_, knn.best_score_)"
      ],
      "metadata": {
        "id": "s5y1gqkpI0Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used cross validation which means that we can see the average and variance of the test score across the different validation splits. We can also view this for each of the parameter combinations."
      ],
      "metadata": {
        "id": "w6fvTur_OIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"n_neighbors |  weights    |     R2 +/- std\")\n",
        "for i in range(len(knn.cv_results_['params'])):\n",
        "  print(f\"{knn.cv_results_['params'][i]['n_neighbors']:4d}   \", end='')\n",
        "  print(f\"{knn.cv_results_['params'][i]['weights']:>15s}     \", end='')\n",
        "  print(f\"{knn.cv_results_['mean_test_score'][i]: 5.3f}  +/- \", end='')\n",
        "  print(f\"{knn.cv_results_['std_test_score'][i]:5.3f}\")"
      ],
      "metadata": {
        "id": "b6ecWvcIJFCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is there a significant difference between the best and worst performing model in the above table?\n",
        "\n",
        "How do you interpret your answer to the above?"
      ],
      "metadata": {
        "id": "zqlhbatNOag4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Retrain our KNN model using the best parameters and recompute the $R^2$ metric. Compare the new score to our expecations from above."
      ],
      "metadata": {
        "id": "xedKePGAOjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the best n_neighbors and weights\n",
        "knn = ?\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = ?\n",
        "r2 = metrics.r2_score(?)\n",
        "print(f'R^2 = {r2}')"
      ],
      "metadata": {
        "id": "mWhE9hTQJIwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to be running more than one model so let us save a summary of the results into a dataframe."
      ],
      "metadata": {
        "id": "_9PHTs0IO0wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltas = pd.DataFrame()\n",
        "deltas['y-knn_y'] = y_test-knn_y_pred\n",
        "deltas.describe()"
      ],
      "metadata": {
        "id": "nbrKP_rSJkcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above data frame doesn't give us a good feeling of how well the model predicts the data, so let's make a plot to visualise it."
      ],
      "metadata": {
        "id": "vL-QPImYO8Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mx = max(y_test.max(), knn_y_pred.max())\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(y_test,knn_y_pred, 'b.')\n",
        "ax.plot([0,mx], [0, mx], 'k--')\n",
        "ax.set_xlabel(\"true\")\n",
        "ax.set_ylabel(\"predicted\")\n",
        "ax.set_title(\"KNN redshift estimation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ruzOm9vXJX0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "In the above plot, you should see that your ML model has done a fairly good job of estimating the redshift for the vast majority of galaxies.\n",
        "The variance is less than 0.1.\n",
        "However, the predictor doesn't seem to want to predict redshifts higher than about 1.0, even though there are some $z~1$ galaxies in our sample.\n",
        "\n",
        "Why might this be the case?"
      ],
      "metadata": {
        "id": "EI53IsG1P3Fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression"
      ],
      "metadata": {
        "id": "qkQxZCKfQFdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Fit a linear regression model (`linear_model.LinearRegression` to our data and predict some redshifts."
      ],
      "metadata": {
        "id": "GeAMjLxIPGpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no need for a grid search as there are no hyper-parameters to tune\n",
        "lr = ?\n",
        "lr.fit(X_train, y_train)\n",
        "lr_y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "AR34usIXJbs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a little fun, let's have a look at the euqation of best fit that was generated by our model."
      ],
      "metadata": {
        "id": "IRzyHUxaPJdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"redshift = \",end='')\n",
        "for feature, coef in zip(X.columns, lr.coef_):\n",
        "  print(f\"{coef:+5.3f}*{feature} \", end='')\n",
        "print(f\"{lr.intercept_:+5.3f}\")"
      ],
      "metadata": {
        "id": "ZjScia4aJga6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "What does it mean for a feature to have a high or low weight in the above equation?\n",
        "\n",
        "If we were using un-scaled data would you have a different answer?"
      ],
      "metadata": {
        "id": "n_kbDns2QM2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mx = max(y_test.max(), lr_y_pred.max())\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(y_test,lr_y_pred, 'b.')\n",
        "ax.plot([0,mx], [0, mx], 'k--')\n",
        "ax.set_xlabel(\"true\")\n",
        "ax.set_ylabel(\"predicted\")\n",
        "ax.set_title(\"Linear Regression redshift estimation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "52FvBP0IJoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "What are some features of the above plot that are different from the KNN version of the plot?"
      ],
      "metadata": {
        "id": "Rfdzl7msQQgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Add a new column to our `deltas` data frame for the linear regression results and compare them to the knn results."
      ],
      "metadata": {
        "id": "tx9uno-nQTQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltas['y-lr_y'] = ?\n",
        "deltas.describe()"
      ],
      "metadata": {
        "id": "vJVunmRsJyPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression trees"
      ],
      "metadata": {
        "id": "q2efEAP1Qgda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Train a `DescisionTreeRegressor` with a range of `max_depth` and `min_samples_split`, using the `r2` scoring metric."
      ],
      "metadata": {
        "id": "cviHPQbwUSUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of all the parameters we'll be iterating over\n",
        "parameters = {'max_depth': [?], # choose a range of values from 2 to 20\n",
        "              'min_samples_split':[ ? ]}\n",
        "# make a classifier object\n",
        "?\n",
        "# create a GridSearchCV object to do the training with cross validation\n",
        "?\n",
        "# now train our model\n",
        "dtr = ?\n",
        "dtr_y_pred = ?\n",
        "print(dtr.best_params_, dtr.best_score_)"
      ],
      "metadata": {
        "id": "ZHVW4ujdRCL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Use the best parameters from above to retrain on all the training data."
      ],
      "metadata": {
        "id": "V5T5LR_1Ugch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_dtr = DecisionTreeRegressor(max_depth=7,\n",
        "                                 min_samples_split=1000)\n",
        "best_dtr.fit(X_train, y_train)\n",
        "dtr_y_pred = best_dtr.predict(X_test)"
      ],
      "metadata": {
        "id": "085nA1AKStxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with our classifier trees, we can ask for a visual representation of the tree.\n",
        "This time the colouring of the nodes represents the value assigned to each group, not the purity of the group."
      ],
      "metadata": {
        "id": "sWFsvtZIUlkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,1, figsize=(18,12))\n",
        "sklearn.tree.plot_tree(best_dtr,\n",
        "               filled=True,\n",
        "               feature_names = X.columns,\n",
        "               rounded=True,\n",
        "               ax=ax,\n",
        "               fontsize=12, )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zk9TMyGjSMc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Inspect the tree above and determine how many unique values of redshift it can produce.\n",
        "\n",
        "Additionally, determine if there are any features that are not used, and review these features in the correlation plot we made earlier. Do you see any agreement?"
      ],
      "metadata": {
        "id": "naGRe2XCUHZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deltas['y-dtr_y'] = ?\n",
        "deltas.describe()"
      ],
      "metadata": {
        "id": "tDlniccyTVmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mx = max(y_test.max(), dtr_y_pred.max())\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(y_test,dtr_y_pred, 'b.')\n",
        "ax.plot([0,mx], [0, mx], 'k--')\n",
        "ax.set_xlabel(\"true\")\n",
        "ax.set_ylabel(\"predicted\")\n",
        "ax.set_title(\"Descision Tree Regression redshift estimation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K-J_vi9QSeDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Challenge](https://cdn.icon-icons.com/icons2/2110/PNG/64/challenge_icon_131034.png)\n",
        "\n",
        "Based on the graph, the $R^2$ metric, and the above table, which of the three models is performing the best?"
      ],
      "metadata": {
        "id": "nZ4flCECQYHW"
      }
    }
  ]
}